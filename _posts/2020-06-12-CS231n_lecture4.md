---
layout : post
category: 정리노트
title: Neural Networks
tags : [CS231n,DeepLearning]
use_math : true
---

이전에 gradient descent 방법에 대해서 개념적으로 다뤄보았습니다. 그렇다면 실제 gradient를 어떻게 구할지 생각해봅시다.

## Computational graphs

gradient를 구하기 전에 먼저 구현의 편리성을 위해서 Computational graphs를 소개합니다. 계산그래프는 numerical한 공식을 그래프로 표현하는 방법입니다. 

<p align="center">
<img width="500" height="250" src="https://drive.google.com/file/d/1rnrGDvPCyd6YmcZsXg72cKQONf3-6dbR/uc?usp=sharing">
</p>

위 그림과 같이 입력, 함수, 출력 등으로 노드화할 수 있으며 backpropagation이라는 방법을 계산하는데 많이 쓰입니다.
각 노드별로 독립적으로 계산할 수 있다는 특징이 있기 때문에 쓰이는 것입니다.

## Backpropagation

Neural Network는 바로 backpropagation방법을 통해서 네트워크를 학습하게 됩니다. 이름에서 볼 수 있는 것과 같이 여러개의 가중치의 조합으로 정답을 예측하고 예측값과 정답간의 오차를 **역으로 가중치들에게 전달** 하는 방식입니다.
가중치를 역으로 전달하는 방식은 chain rule을 이용합니다. chain rule이란 다음 수식과 같습니다.

$$\frac{\partial{f}}{\partial{y}} = \frac{\partial{f}}{\partial{q}}\frac{\partial{q}}{\partial{y}}$$

위 수식과 같이 미분의 곱의 연쇄작용으로 최종 미분값을 구할 수 있게 됩니다. 결국 입력(x)에 대한 gradient를 각 노드(가중치)에 대한 gradient의 곱 연산으로 나타낼 수 있고, 그 gradient를  이용하여 가중치들을 업데이트하는 원리입니다.



## Multiclass SVM loss


출처
