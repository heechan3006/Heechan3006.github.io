---
layout : post
category: 정리노트
title: Loss Functions and Opimization
tags : [CS231n,DeepLearning]
use_math : true
---

## Linear Classification

이전 포스트에서 설명했던 kNN은 영상분류를 할때에 한계점이 있었습니다.

이런 한계점을 극복하기 위해서 다른 방식의 분류기를 설명하고자 합니다.

$$f(x_i, W, b) =  W x_i + b$$

위의 방정식은 linear classifer의 방정식입니다.

$x_i$를 영상픽셀값을 일렬로 나열한 single column vector라고 합시다.

W(Weights)와 b(bias)가 우리가 구해야하는 방정식의 파라미터입니다.

이 방정식을 이용하여 N개의 클래스를 가진 분류기를 만들기 위해서는 N개의 분류기를 각각 만들어야 합니다.

N개의 방정식 값을 비교하여 가장 높은 값이 원하는 클래스가 되도록 해야합니다.

이를 수행하기 위해서는

1. define loss function 
2. opimization

의 과정이 필요합니다.

## Multiclass SVM loss

사용가능한 loss function중 하나가 multiclass SVM loss입니다. 

$$ L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta) $$

loss 방정식은 위와 같습니다. s는 linear classifier방정식의 결과값입니다.

![](https://drive.google.com/uc?id=1qadsb79hUZtaqz8uswnyq-j0d6kQqOFF)

위의 그림에서 보여지는 것과같이 상수 $\Delta$로 correct class의 점수 margin을 조절합니다.

그리고, 원래의 정답 클래스의 score와 다른 클래스의 score의 차이를 각각 모두 더한 값이 최종 loss가 됩니다.

그래프에서 보는것과같이 정답클래스의 score가 상대적으로 다른 클래스의 score보다 높기만 하면 되는 구조로 되어있습니다.

따라서, 일정 score이상이면 loss가 0이 되는 것을 볼 수 있습니다.

## Softmax Classifier (Multinomial Logistic Regression)

다음으로 소개할 loss function은 Softmax Classifier입니다.

![](https://drive.google.com/uc?id=1H4TZdvHxe_9dSLGzuJ2OKuAuDqPO1gmc)

각각의 weights들을 확률화하는 방식입니다.

수식은 다음과 같습니다.

$$P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }\tag{Softmax function}$$

$$L_i = -logP(Y=y_i|X=x_i) \tag{Maximum likelihood}$$
Softmax로 확률을 구하고, maximum likelihood를 이용한 것이 Softmax Classifier입니다.

maximum likelihood를 사용하는 이유는 여러가지가 있습니다.

이를 설명하기 위해서는 우선, 베이즈 정리의 개념을 알아야 합니다.

$$
P(A|B) = \dfrac{P(B|A)P(A)}{P(B)}
$$

베이즈 정리는 데이터라는 조건이 주어졌을 때의 조건부 확률을 구하는 공식입니다.

사건 B가 발생하기 전에 알고있는 사건 A의 사전확률을 이용하여 사건 A의 사후확률을 구하는 것입니다.

갱신된 사건 A의 사후확률을 구하기 위해서는 $P(B|A)$인 가능도를 이용합니다.

가능도(likelihood)란 사건 A가 발생한 경우 B의 확률입니다.

영상분류와 매칭을 시켜보면, 예측한 값이 정답일 확률을 높힘으로써 한번도 보지 않은 영상을 예측했을 때 그것이 정답일 확률을 높이고자 하는 것입니다.

또한 likelihood를 사용함으로써, 정답클래스로 예측한 확률만을 이용하여 오차를 최소화시킬 수 있습니다.

이러한 이유로 likelihood를 학습하는것인데, 왜 log를 취할까요?

첫번째로, 확률을 계속 곱해서 나타나는 underflow를 방지할 수 있습니다.

두번째로, 첫번째와 비슷한 맥락인데 곱을 합으로 변환시켜줍니다. 즉, 미분이 용이합니다.

다음 챕터에서 설명하겠지만, 오차를 최소화시키는 optimization을 하기 위해서 편미분을 해야합니다. log를 이용하면 이런 편미분 과정을 간단하게 처리할 수 있습니다.



출처

[](https://cs231n.github.io/)
[](https://datascienceschool.net/view-notebook/f68d16df9ea448689ae66dc2140fe673/)
