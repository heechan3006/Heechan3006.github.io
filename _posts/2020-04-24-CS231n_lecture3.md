---
layout : post
category: 정리노트
title: Loss Functions and Opimization
tags : [CS231n,DeepLearning]
use_math : true
---

## Linear Classification

이전 포스트에서 설명했던 kNN은 영상분류를 할때에 한계점이 있었습니다.

이런 한계점을 극복하기 위해서 다른 방식의 분류기를 설명하고자 합니다.

$$f(x_i, W, b) =  W x_i + b$$

위의 방정식은 linear classifer의 방정식입니다.

$x_i$를 영상픽셀값을 일렬로 나열한 single column vector라고 합시다.

W(Weights)와 b(bias)가 우리가 구해야하는 방정식의 파라미터입니다.

이 방정식을 이용하여 N개의 클래스를 가진 분류기를 만들기 위해서는 N개의 분류기를 각각 만들어야 합니다.

N개의 방정식 값을 비교하여 가장 높은 값이 원하는 클래스가 되도록 해야합니다.

이를 수행하기 위해서는

1. define loss function 
2. opimization

의 과정이 필요합니다.

## Multiclass SVM loss

사용가능한 loss function중 하나가 multiclass SVM loss입니다. 

$$ L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta) $$

loss 방정식은 위와 같습니다. s는 linear classifier방정식의 결과값입니다.

<p align="center">
<img width="400" height="300" src ="https://drive.google.com/uc?id=1qadsb79hUZtaqz8uswnyq-j0d6kQqOFF">
</p>

위의 그림에서 보여지는 것과같이 상수 $\Delta$로 correct class의 점수 margin을 조절합니다.

그리고, 원래의 정답 클래스의 score와 다른 클래스의 score의 차이를 각각 모두 더한 값이 최종 loss가 됩니다.

그래프에서 보는것과같이 정답클래스의 score가 상대적으로 다른 클래스의 score보다 높기만 하면 되는 구조로 되어있습니다.

따라서, 일정 score이상이면 loss가 0이 되는 것을 볼 수 있습니다.

## Softmax Classifier (Multinomial Logistic Regression)

다음으로 소개할 loss function은 Softmax Classifier입니다.

<p align="center">
<img width="600" height="300" src="https://drive.google.com/uc?id=1H4TZdvHxe_9dSLGzuJ2OKuAuDqPO1gmc">
</p>

각각의 weights들을 확률화하는 방식입니다.

수식은 다음과 같습니다.

$$
P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }\tag{Softmax function}
$$

$$L_i = -logP(Y=y_i|X=x_i) \tag{Maximum likelihood}$$
Softmax로 확률을 구하고, maximum likelihood를 이용한 것이 Softmax Classifier입니다.

maximum likelihood를 사용하는 이유는 여러가지가 있습니다.

이를 설명하기 위해서는 우선, 베이즈 정리의 개념을 알아야 합니다.

$$
P(A|B) = \dfrac{P(B|A)P(A)}{P(B)}\tag{Bayes' theorem}
$$

베이즈 정리는 데이터라는 조건이 주어졌을 때의 조건부 확률을 구하는 공식입니다.

사건 B가 발생하기 전에 알고있는 사건 A의 사전확률을 이용하여 사건 A의 사후확률을 구하는 것입니다.

갱신된 사건 A의 사후확률을 구하기 위해서는 $ P(B{|}A) $인 가능도를 이용합니다.

가능도(likelihood)란 사건 A가 발생한 경우 B의 확률입니다.

영상분류와 매칭을 시켜보면, 예측한 값이 정답일 확률을 높힘으로써 한번도 보지 않은 영상을 예측했을 때 그것이 정답일 확률을 높이고자 하는 것입니다.

또한 likelihood를 사용함으로써, 정답클래스로 예측한 확률만을 이용하여 오차를 최소화시킬 수 있습니다.

이러한 이유로 likelihood를 학습하는것인데, 왜 log를 취할까요?

첫번째로, 확률을 계속 곱해서 나타나는 underflow를 방지할 수 있습니다.

두번째로, 첫번째와 비슷한 맥락인데 곱을 합으로 변환시켜줍니다. 즉, 미분이 용이합니다.

다음 챕터에서 설명하겠지만, 오차를 최소화시키는 optimization을 하기 위해서 편미분을 해야합니다. log를 이용하면 이런 편미분 과정을 간단하게 처리할 수 있습니다.

## Softmax vs SVM

지금까지 Softmax와 SVM에 대해서 설명하였습니다.

이 두 loss function의 차이점은 무엇일까요?

SVM의 경우에는 correct class가 incorrect class보다 크면 어떤 Weight든지 상관이 없습니다.

그러나, Softmax의 경우에는 correct class의 Probability가 1이 될때까지 최선의 Weight를 찾습니다. 이러한 관점의 차이가 있습니다.

어떤 loss function이 좋다 나쁘다라고 단정지을 수 없지만, 어떤 task를 다루는지에 따라서 두개의 loss function중 하나를 선택할 수 있을 것입니다.

## Regularization

<p align="center">
<img width="450" height="300" src="https://drive.google.com/uc?id=1Cw_evWPyXEKF-3j32BRoc4Y1PPdtrEEJ">
</p>
Weight를 학습할 때, 가장 중요한 것은 training data를 fitting시키는것이 아니고, test data를 잘 맞추는 것입니다.

위에서 사용한 loss function으로 학습을 하면 위 그림의 파란색 선처럼 fitting이 될 것입니다.

그러나, 정작 우리가 원하는 것은 초록색 선과같이 fitting이 되는 것입니다.

이는 고차원의 가중치가 반영이 되어서 함수 자체가 복잡해지는데에 있습니다.

이런 고차원함수를 저차원으로 낮춤으로써 흔히말하는 과적합(over fitting)을 방지할 수 있습니다.

저차원으로 어떻게 낮출까요?

우리가 데이터를 건드리지 않고 할 수 있는 일은 Weight를 이용하는 것입니다.

$$ W_0+ W_1{x} + W_2{x^2} + W_3{x^3}+...+W_k{x^k}$$

기본 컨셉은 고차원의 매개변수에 큰값의 패널티를 주어서 Weight자체를 감소시킴으로써 저차원의 함수로 변환시킵니다.

고차원 Weight에 큰 값을 곱하여 결과값이 최소가 되게 하면 저차원의 weight들만 

출처

[](https://cs231n.github.io/)
[](https://datascienceschool.net/view-notebook/f68d16df9ea448689ae66dc2140fe673/)
